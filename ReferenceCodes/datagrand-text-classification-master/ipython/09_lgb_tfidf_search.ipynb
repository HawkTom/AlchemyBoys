{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "from time import time\n",
    "from pprint import pprint\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import f1_score, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load data...\n",
      "The number of samples is: 4999\n",
      "The number of classes is: 19\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load data\n",
    "# ============================================================================\n",
    "\n",
    "print(\"Load data...\")\n",
    "data_path = \"../raw_data/train_demo.csv\"\n",
    "data = pd.read_csv(data_path)\n",
    "\n",
    "X_text = data['word_seg']\n",
    "y = data['class'] - 1\n",
    "num_classes = max(y) + 1\n",
    "print(\"The number of samples is: %d\" % len(X_text))\n",
    "print(\"The number of classes is: %d\" % num_classes)\n",
    "\n",
    "del data\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorizer's hyper-parameters:\n",
      "{'max_df': 0.9,\n",
      " 'max_features': None,\n",
      " 'min_df': 3,\n",
      " 'ngram_range': (1, 2),\n",
      " 'sublinear_tf': True}\n",
      "Extract features...\n",
      "Done in 16.612 seconds.\n",
      "Extract finished! ( ^ _ ^ ) V\n",
      "The number of features is 190239\n"
     ]
    }
   ],
   "source": [
    "# Extract feature vectors\n",
    "# ============================================================================\n",
    "\n",
    "# Extractor's hyper-parameters\n",
    "vect_params = {\n",
    "    'ngram_range': (1, 2),\n",
    "    'max_df': 0.9,\n",
    "    'min_df': 3,\n",
    "    'max_features': None,\n",
    "    'sublinear_tf': True\n",
    "}\n",
    "print(\"Vectorizer's hyper-parameters:\")\n",
    "pprint(vect_params)\n",
    "\n",
    "# Initialize feature extractor\n",
    "vectorizer = TfidfVectorizer(**vect_params)\n",
    "\n",
    "print(\"Extract features...\")\n",
    "t0_extract = time()\n",
    "X = vectorizer.fit_transform(X_text)\n",
    "print(\"Done in %.3f seconds.\" % (time() - t0_extract))\n",
    "print(\"Extract finished! ( ^ _ ^ ) V\")\n",
    "\n",
    "# The number of features\n",
    "num_feats = len(vectorizer.get_feature_names())\n",
    "print(\"The number of features is %d\" % num_feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split data into training and validation set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\envs\\py3_for_prac\\lib\\site-packages\\sklearn\\model_selection\\_split.py:2026: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# Split data into training and validation set\n",
    "# ============================================================================\n",
    "\n",
    "print(\"Split data into training and validation set...\")\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, train_size=0.8, random_state=42)\n",
    "\n",
    "lgb_train = lgb.Dataset(X_train, y_train)\n",
    "lgb_val = lgb.Dataset(X_val, y_val, reference=lgb_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of parameter combinations is: 1\n",
      "\n",
      "Parameter combination 1\n",
      "LightGBM's hyper-parameters:\n",
      "{'bagging_fraction': 0.9,\n",
      " 'bagging_freq': 1,\n",
      " 'boosting_type': 'gbdt',\n",
      " 'feature_fraction': 0.8,\n",
      " 'is_training_metric': 'True',\n",
      " 'lambda_l1': 0,\n",
      " 'lambda_l2': 100,\n",
      " 'learning_rate': 0.05,\n",
      " 'max_depth': 10,\n",
      " 'metric': 'multi_logloss',\n",
      " 'min_data_in_leaf': 50,\n",
      " 'min_gain_to_split': 0,\n",
      " 'min_sum_hessian_in_leaf': 0.1,\n",
      " 'num_class': 19,\n",
      " 'num_leaves': 127,\n",
      " 'num_threads': 16,\n",
      " 'objective': 'multiclass',\n",
      " 'verbose': 0}\n",
      "Start training...\n",
      "Training until validation scores don't improve for 50 rounds.\n",
      "[100]\ttrain's multi_logloss: 1.45408\tval's multi_logloss: 1.76493\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\ttrain's multi_logloss: 1.45408\tval's multi_logloss: 1.76493\n",
      "Done in 151.519 seconds\n",
      "Training finished! ( ^ _ ^ ) V\n",
      "Best iteration: 100\n",
      "Training Loss: 1.45408, Validation Loss: 1.76493\n",
      "Training Accuracy: 72.22, Validation Accuracy: 58.20\n",
      "Training F1 Score: 0.71664, Validation F1 Score: 0.57497\n"
     ]
    }
   ],
   "source": [
    "# Tuning the hyper-parameters of LightGBM model and save the results\n",
    "# ============================================================================\n",
    "\n",
    "df_params = pd.read_csv(\"lgb-tfidf-params-2.csv\")\n",
    "num_params = df_params.shape[0]\n",
    "print(\"The number of parameter combinations is: %d\" % num_params)\n",
    "\n",
    "for i in range(num_params):\n",
    "    print()\n",
    "    print(\"Parameter combination %d\" % (i + 1))\n",
    "    \n",
    "    gbm_params = {\n",
    "        'boosting_type': df_params['type'].values[i],\n",
    "        'objective': 'multiclass',\n",
    "        'num_class': num_classes,\n",
    "        'metric': 'multi_logloss',\n",
    "\n",
    "        'learning_rate': df_params['lr'].values[i],\n",
    "\n",
    "        'num_leaves': df_params['n_leaf'].values[i],\n",
    "        'max_depth': df_params['n_depth'].values[i],\n",
    "        'min_data_in_leaf': df_params['min_data'].values[i],\n",
    "\n",
    "        'feature_fraction': df_params['feat_frac'].values[i],\n",
    "        'bagging_fraction': df_params['bagging_frac'].values[i],\n",
    "        'bagging_freq': df_params['bagging_freq'].values[i],\n",
    "\n",
    "        'lambda_l1': df_params['l1'].values[i],\n",
    "        'lambda_l2': df_params['l2'].values[i],\n",
    "        'min_gain_to_split': df_params['min_gain'].values[i],\n",
    "        'min_sum_hessian_in_leaf': df_params['hessian'].values[i],\n",
    "\n",
    "        'num_threads': 16,\n",
    "        'verbose': 0,\n",
    "        'is_training_metric': 'True'\n",
    "    }\n",
    "    print(\"LightGBM's hyper-parameters:\")\n",
    "    pprint(gbm_params)\n",
    "\n",
    "    print(\"Start training...\")\n",
    "    t0_train = time()\n",
    "    evals_result = {}\n",
    "    gbm = lgb.train(params=gbm_params,\n",
    "                    train_set=lgb_train,\n",
    "                    num_boost_round=100,\n",
    "                    valid_sets=[lgb_train, lgb_val],\n",
    "                    valid_names=['train', 'val'],\n",
    "                    evals_result=evals_result,\n",
    "                    early_stopping_rounds=50,\n",
    "                    verbose_eval=100)\n",
    "    print(\"Done in %.3f seconds\" % (time() - t0_extract))\n",
    "    print(\"Training finished! ( ^ _ ^ ) V\")\n",
    "    \n",
    "    best_iter = gbm.best_iteration\n",
    "    loss_train = evals_result['train']['multi_logloss'][best_iter-1]\n",
    "    loss_val = evals_result['val']['multi_logloss'][best_iter-1]\n",
    "\n",
    "    probs_train = gbm.predict(X_train, num_iteration=best_iter)\n",
    "    preds_train = np.argmax(probs_train, axis=1)\n",
    "    acc_train = accuracy_score(y_train, preds_train)\n",
    "    f1_train = f1_score(y_train, preds_train, average='weighted')\n",
    "\n",
    "    probs_val = gbm.predict(X_val, num_iteration=best_iter)\n",
    "    preds_val = np.argmax(probs_val, axis=1)\n",
    "    acc_val = accuracy_score(y_val, preds_val)\n",
    "    f1_val = f1_score(y_val, preds_val, average='weighted')\n",
    "\n",
    "    print(\"Best iteration: %d\" % best_iter)\n",
    "    print(\"Training Loss: %.5f, Validation Loss: %.5f\" % (loss_train, loss_val))\n",
    "    print(\"Training Accuracy: %.2f, Validation Accuracy: %.2f\" % (acc_train * 100, acc_val * 100))\n",
    "    print(\"Training F1 Score: %.5f, Validation F1 Score: %.5f\" % (f1_train, f1_val))\n",
    "    \n",
    "    res = \"%s,%s,%d,%s,%f,%d,%s,%s,%s,%.4f,%d,%d,%d,%.4f,%.4f,%d,%.4e,%.4e,%.4e,%.4e,%.4e,%d,%.5f,%.5f,%.2f,%.2f,%.5f,%.5f\\n\" % (\n",
    "        datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "        \"lgb-tfidf-%d\" % (i + 1),  # model name\n",
    "        num_feats,  # number of features\n",
    "        '1|2',\n",
    "        0.9,\n",
    "        3,\n",
    "        'None',\n",
    "        'True',\n",
    "        gbm_params['boosting_type'],\n",
    "        gbm_params['learning_rate'],\n",
    "        gbm_params['num_leaves'],\n",
    "        gbm_params['max_depth'],\n",
    "        gbm_params['min_data_in_leaf'],\n",
    "        gbm_params['feature_fraction'],\n",
    "        gbm_params['bagging_fraction'],\n",
    "        gbm_params['bagging_freq'],\n",
    "        gbm_params['lambda_l1'],\n",
    "        gbm_params['lambda_l2'],\n",
    "        gbm_params['min_gain_to_split'],\n",
    "        gbm_params['min_sum_hessian_in_leaf'],\n",
    "        0.8,  # train size\n",
    "        best_iter,  # best iteration\n",
    "        loss_train,  # multi-logloss of training set\n",
    "        loss_val,  # multi-logloss of validation set\n",
    "        acc_train,  # accuracy of training set\n",
    "        acc_val,  # accuracy of validation set\n",
    "        f1_train,  # f1 score of training set\n",
    "        f1_val  # f1 score of validation set\n",
    "    )\n",
    "\n",
    "    f = open(\"lgb-tfidf-tuning-results.csv\", 'a')\n",
    "    f.write(res)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
