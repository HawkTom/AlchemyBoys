{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\envs\\py3_for_prac\\lib\\site-packages\\sklearn\\model_selection\\_split.py:2026: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "data_path = \"../processed_data/train-data-300d-sum.txt\"\n",
    "data = pd.read_csv(data_path)\n",
    "\n",
    "X = data.drop(['class'], axis=1)\n",
    "y = data['class'] - 1\n",
    "num_feats = X.shape[1]\n",
    "num_classes = max(y) + 1\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, train_size=0.8, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyper-parameters:\n",
      "{'boosting_type': 'gbdt', 'objective': 'multiclass', 'num_class': 19, 'metric': 'multi_logloss', 'learning_rate': 0.1, 'num_leaves': 31, 'max_depth': 6, 'min_data_in_leaf': 512, 'feature_fraction': 0.6, 'bagging_fraction': 0.9, 'bagging_freq': 1, 'lambda_l1': 6.37, 'lambda_l2': 65200, 'min_gain_to_split': 0, 'min_sum_hessian_in_leaf': 0.1, 'num_threads': 16, 'verbose': 0, 'is_training_metric': 'True'}\n",
      "[1]\ttrain's multi_logloss: 2.94427\tval's multi_logloss: 2.94428\n",
      "Training until validation scores don't improve for 50 rounds.\n",
      "[2]\ttrain's multi_logloss: 2.94411\tval's multi_logloss: 2.94413\n",
      "[3]\ttrain's multi_logloss: 2.94394\tval's multi_logloss: 2.94397\n",
      "[4]\ttrain's multi_logloss: 2.94377\tval's multi_logloss: 2.94381\n",
      "[5]\ttrain's multi_logloss: 2.94361\tval's multi_logloss: 2.94366\n",
      "[6]\ttrain's multi_logloss: 2.94345\tval's multi_logloss: 2.94351\n",
      "[7]\ttrain's multi_logloss: 2.94328\tval's multi_logloss: 2.94336\n",
      "[8]\ttrain's multi_logloss: 2.94311\tval's multi_logloss: 2.94321\n",
      "[9]\ttrain's multi_logloss: 2.94295\tval's multi_logloss: 2.94305\n",
      "[10]\ttrain's multi_logloss: 2.94279\tval's multi_logloss: 2.9429\n",
      "[11]\ttrain's multi_logloss: 2.94262\tval's multi_logloss: 2.94274\n",
      "[12]\ttrain's multi_logloss: 2.94246\tval's multi_logloss: 2.94259\n",
      "[13]\ttrain's multi_logloss: 2.94229\tval's multi_logloss: 2.94244\n",
      "[14]\ttrain's multi_logloss: 2.94213\tval's multi_logloss: 2.94228\n",
      "[15]\ttrain's multi_logloss: 2.94196\tval's multi_logloss: 2.94213\n",
      "[16]\ttrain's multi_logloss: 2.9418\tval's multi_logloss: 2.94198\n",
      "[17]\ttrain's multi_logloss: 2.94163\tval's multi_logloss: 2.94182\n",
      "[18]\ttrain's multi_logloss: 2.94146\tval's multi_logloss: 2.94167\n",
      "[19]\ttrain's multi_logloss: 2.9413\tval's multi_logloss: 2.94151\n",
      "[20]\ttrain's multi_logloss: 2.94113\tval's multi_logloss: 2.94136\n",
      "[21]\ttrain's multi_logloss: 2.94097\tval's multi_logloss: 2.9412\n",
      "[22]\ttrain's multi_logloss: 2.9408\tval's multi_logloss: 2.94105\n",
      "[23]\ttrain's multi_logloss: 2.94063\tval's multi_logloss: 2.94089\n",
      "[24]\ttrain's multi_logloss: 2.94047\tval's multi_logloss: 2.94074\n",
      "[25]\ttrain's multi_logloss: 2.94031\tval's multi_logloss: 2.94058\n",
      "[26]\ttrain's multi_logloss: 2.94015\tval's multi_logloss: 2.94044\n",
      "[27]\ttrain's multi_logloss: 2.93998\tval's multi_logloss: 2.94028\n",
      "[28]\ttrain's multi_logloss: 2.93982\tval's multi_logloss: 2.94013\n",
      "[29]\ttrain's multi_logloss: 2.93965\tval's multi_logloss: 2.93998\n",
      "[30]\ttrain's multi_logloss: 2.93949\tval's multi_logloss: 2.93983\n",
      "[31]\ttrain's multi_logloss: 2.93933\tval's multi_logloss: 2.93967\n",
      "[32]\ttrain's multi_logloss: 2.93917\tval's multi_logloss: 2.93953\n",
      "[33]\ttrain's multi_logloss: 2.939\tval's multi_logloss: 2.93938\n",
      "[34]\ttrain's multi_logloss: 2.93884\tval's multi_logloss: 2.93922\n",
      "[35]\ttrain's multi_logloss: 2.93867\tval's multi_logloss: 2.93907\n",
      "[36]\ttrain's multi_logloss: 2.93851\tval's multi_logloss: 2.93892\n",
      "[37]\ttrain's multi_logloss: 2.93834\tval's multi_logloss: 2.93876\n",
      "[38]\ttrain's multi_logloss: 2.93817\tval's multi_logloss: 2.9386\n",
      "[39]\ttrain's multi_logloss: 2.938\tval's multi_logloss: 2.93844\n",
      "[40]\ttrain's multi_logloss: 2.93784\tval's multi_logloss: 2.93829\n",
      "[41]\ttrain's multi_logloss: 2.93767\tval's multi_logloss: 2.93813\n",
      "[42]\ttrain's multi_logloss: 2.93751\tval's multi_logloss: 2.93798\n",
      "[43]\ttrain's multi_logloss: 2.93735\tval's multi_logloss: 2.93783\n",
      "[44]\ttrain's multi_logloss: 2.93718\tval's multi_logloss: 2.93768\n",
      "[45]\ttrain's multi_logloss: 2.93702\tval's multi_logloss: 2.93752\n",
      "[46]\ttrain's multi_logloss: 2.93685\tval's multi_logloss: 2.93737\n",
      "[47]\ttrain's multi_logloss: 2.93669\tval's multi_logloss: 2.93722\n",
      "[48]\ttrain's multi_logloss: 2.93652\tval's multi_logloss: 2.93707\n",
      "[49]\ttrain's multi_logloss: 2.93636\tval's multi_logloss: 2.93692\n",
      "[50]\ttrain's multi_logloss: 2.93619\tval's multi_logloss: 2.93676\n",
      "[51]\ttrain's multi_logloss: 2.93603\tval's multi_logloss: 2.93661\n",
      "[52]\ttrain's multi_logloss: 2.93586\tval's multi_logloss: 2.93645\n",
      "[53]\ttrain's multi_logloss: 2.9357\tval's multi_logloss: 2.9363\n",
      "[54]\ttrain's multi_logloss: 2.93553\tval's multi_logloss: 2.93615\n",
      "[55]\ttrain's multi_logloss: 2.93537\tval's multi_logloss: 2.936\n",
      "[56]\ttrain's multi_logloss: 2.93521\tval's multi_logloss: 2.93585\n",
      "[57]\ttrain's multi_logloss: 2.93505\tval's multi_logloss: 2.93569\n",
      "[58]\ttrain's multi_logloss: 2.93488\tval's multi_logloss: 2.93554\n",
      "[59]\ttrain's multi_logloss: 2.93472\tval's multi_logloss: 2.93539\n",
      "[60]\ttrain's multi_logloss: 2.93456\tval's multi_logloss: 2.93524\n",
      "[61]\ttrain's multi_logloss: 2.9344\tval's multi_logloss: 2.93508\n",
      "[62]\ttrain's multi_logloss: 2.93424\tval's multi_logloss: 2.93493\n",
      "[63]\ttrain's multi_logloss: 2.93407\tval's multi_logloss: 2.93477\n",
      "[64]\ttrain's multi_logloss: 2.93391\tval's multi_logloss: 2.93463\n",
      "[65]\ttrain's multi_logloss: 2.93374\tval's multi_logloss: 2.93447\n",
      "[66]\ttrain's multi_logloss: 2.93358\tval's multi_logloss: 2.93432\n",
      "[67]\ttrain's multi_logloss: 2.93342\tval's multi_logloss: 2.93417\n",
      "[68]\ttrain's multi_logloss: 2.93326\tval's multi_logloss: 2.93402\n",
      "[69]\ttrain's multi_logloss: 2.93309\tval's multi_logloss: 2.93386\n",
      "[70]\ttrain's multi_logloss: 2.93293\tval's multi_logloss: 2.93371\n",
      "[71]\ttrain's multi_logloss: 2.93276\tval's multi_logloss: 2.93356\n",
      "[72]\ttrain's multi_logloss: 2.93259\tval's multi_logloss: 2.9334\n",
      "[73]\ttrain's multi_logloss: 2.93243\tval's multi_logloss: 2.93325\n",
      "[74]\ttrain's multi_logloss: 2.93227\tval's multi_logloss: 2.9331\n",
      "[75]\ttrain's multi_logloss: 2.93211\tval's multi_logloss: 2.93295\n",
      "[76]\ttrain's multi_logloss: 2.93195\tval's multi_logloss: 2.9328\n",
      "[77]\ttrain's multi_logloss: 2.93178\tval's multi_logloss: 2.93265\n",
      "[78]\ttrain's multi_logloss: 2.93162\tval's multi_logloss: 2.9325\n",
      "[79]\ttrain's multi_logloss: 2.93146\tval's multi_logloss: 2.93236\n",
      "[80]\ttrain's multi_logloss: 2.9313\tval's multi_logloss: 2.9322\n",
      "[81]\ttrain's multi_logloss: 2.93114\tval's multi_logloss: 2.93206\n",
      "[82]\ttrain's multi_logloss: 2.93097\tval's multi_logloss: 2.9319\n",
      "[83]\ttrain's multi_logloss: 2.93081\tval's multi_logloss: 2.93174\n",
      "[84]\ttrain's multi_logloss: 2.93065\tval's multi_logloss: 2.9316\n",
      "[85]\ttrain's multi_logloss: 2.93049\tval's multi_logloss: 2.93145\n",
      "[86]\ttrain's multi_logloss: 2.93032\tval's multi_logloss: 2.9313\n",
      "[87]\ttrain's multi_logloss: 2.93016\tval's multi_logloss: 2.93114\n",
      "[88]\ttrain's multi_logloss: 2.92999\tval's multi_logloss: 2.93098\n",
      "[89]\ttrain's multi_logloss: 2.92983\tval's multi_logloss: 2.93083\n",
      "[90]\ttrain's multi_logloss: 2.92967\tval's multi_logloss: 2.93068\n",
      "[91]\ttrain's multi_logloss: 2.92951\tval's multi_logloss: 2.93053\n",
      "[92]\ttrain's multi_logloss: 2.92934\tval's multi_logloss: 2.93038\n",
      "[93]\ttrain's multi_logloss: 2.92919\tval's multi_logloss: 2.93023\n",
      "[94]\ttrain's multi_logloss: 2.92903\tval's multi_logloss: 2.93008\n",
      "[95]\ttrain's multi_logloss: 2.92886\tval's multi_logloss: 2.92992\n",
      "[96]\ttrain's multi_logloss: 2.9287\tval's multi_logloss: 2.92978\n",
      "[97]\ttrain's multi_logloss: 2.92853\tval's multi_logloss: 2.92962\n",
      "[98]\ttrain's multi_logloss: 2.92837\tval's multi_logloss: 2.92946\n",
      "[99]\ttrain's multi_logloss: 2.92821\tval's multi_logloss: 2.92932\n",
      "[100]\ttrain's multi_logloss: 2.92805\tval's multi_logloss: 2.92916\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\ttrain's multi_logloss: 2.92805\tval's multi_logloss: 2.92916\n",
      "Best round: 99\n",
      "Training Loss: 2.92805, Validation Loss: 2.92916\n",
      "Training F1 Score: 0.29000, Validation F1 Score: 0.27654\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\envs\\py3_for_prac\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "# Tuning the hyper-parameters of LightGBM model and save the results\n",
    "lgb_train = lgb.Dataset(X_train.values, y_train.values)\n",
    "lgb_val = lgb.Dataset(X_val.values, y_val.values, reference=lgb_train)\n",
    "\n",
    "df_params = pd.read_csv(\"lgb-params.csv\")\n",
    "num_params = df_params.shape[0]\n",
    "for i in range(num_params):\n",
    "    params = {\n",
    "        'boosting_type': df_params['type'].values[i],\n",
    "        'objective': 'multiclass',\n",
    "        'num_class': num_classes,\n",
    "        'metric': 'multi_logloss',\n",
    "\n",
    "        'learning_rate': df_params['lr'].values[i],\n",
    "\n",
    "        'num_leaves': df_params['n_leaf'].values[i],\n",
    "        'max_depth': df_params['n_depth'].values[i],\n",
    "        'min_data_in_leaf': df_params['min_data'].values[i],\n",
    "\n",
    "        'feature_fraction': df_params['feat_frac'].values[i],\n",
    "        'bagging_fraction': df_params['bagging_frac'].values[i],\n",
    "        'bagging_freq': df_params['bagging_freq'].values[i],\n",
    "\n",
    "        'lambda_l1': df_params['l1'].values[i],\n",
    "        'lambda_l2': df_params['l2'].values[i],\n",
    "        'min_gain_to_split': df_params['min_gain'].values[i],\n",
    "        'min_sum_hessian_in_leaf': df_params['hessian'].values[i],\n",
    "\n",
    "        'num_threads': 16,\n",
    "        'verbose': 0,\n",
    "        'is_training_metric': 'True'\n",
    "    }\n",
    "\n",
    "    print(\"Hyper-parameters:\")\n",
    "    print(params)\n",
    "\n",
    "    evals_result = {}\n",
    "    gbm = lgb.train(params=params,\n",
    "                    train_set=lgb_train,\n",
    "                    num_boost_round=100,\n",
    "                    valid_sets=[lgb_train, lgb_val],\n",
    "                    valid_names=['train', 'val'],\n",
    "                    evals_result=evals_result,\n",
    "                    early_stopping_rounds=50,\n",
    "                    verbose_eval=1)\n",
    "\n",
    "    best_round = gbm.best_iteration - 1\n",
    "    loss_train = evals_result['train']['multi_logloss'][best_round]\n",
    "    loss_val = evals_result['val']['multi_logloss'][best_round]\n",
    "\n",
    "    probs_train = gbm.predict(X_train, num_iteration=best_round)\n",
    "    preds_train = np.argmax(probs_train, axis=1)\n",
    "    f1_train = f1_score(y_train, preds_train, average='weighted')\n",
    "\n",
    "    probs_val = gbm.predict(X_val, num_iteration=best_round)\n",
    "    preds_val = np.argmax(probs_val, axis=1)\n",
    "    f1_val = f1_score(y_val, preds_val, average='weighted')\n",
    "\n",
    "    print(\"Best round: %d\" % best_round)\n",
    "    print(\"Training Loss: %.5f, Validation Loss: %.5f\" % (loss_train, loss_val))\n",
    "    print(\"Training F1 Score: %.5f, Validation F1 Score: %.5f\" % (f1_train, f1_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
